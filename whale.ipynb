{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so there are a lot of whales that we have just one of\n",
    "#let's see how many of these bad boys are compared to the total\n",
    "whales = train.groupby('Id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.670929070929071"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "to get a better idea of what we;re dealing with let's make a histrogram of the whales\n",
    "'''\n",
    "\n",
    "\n",
    "float(whales[whales <= 2].size)/float(whales.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nso it's pretty damn impressive if we can even \\nget 60% of these, seeing as we only have 1 for many of the individuals\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so it's pretty damn impressive if we can even \n",
    "get 60% of these, seeing as we only have 1 for many of the individuals\n",
    "\n",
    "If we're going to learn meaningful features about individuals with only \n",
    "one picture, we can only really learn differences in a meaningful way.  For\n",
    "example, if we see the right side of a fluke, we won't be able to say \n",
    "much about the left side of a fluke of the same individual if say, the whale\n",
    "was partially eaten by a shark.  Although learning differences won;t get us\n",
    "around an example as pedantic as this, it will outperform a standard feature\n",
    "learning network \n",
    "\n",
    "http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf Koch's masters thesis \n",
    "describes training on MNIST data and applying affine transformations.  In \n",
    "theory it could be useful to apply a number of transformations to the whales\n",
    "to see what could happen, but in the first round of training, for the sake, we will \n",
    "design a pipeline that just grayscales the data and randomly selects a pair of a whale \n",
    "with only one member and pairs it with a whale we have multiples of. \n",
    "\n",
    "I'm going to make a class that will let us determine which images to pair and store \n",
    "the pairs as strings we can load into \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.image as img\n",
    "import keras as k\n",
    "import numpy as np\n",
    "import random as r\n",
    "import pandas as pd\n",
    "#tf.enable_eager_execution()\n",
    "'''\n",
    "I'm going to start by creating a generator that pairs elements of small classes with \n",
    "elements of larger classes.  My intent is to train a siamese CNN pair that specifically\n",
    "identifies known whales with only a few members well.\n",
    "'''\n",
    "class TrainingMatcher:\n",
    "    '''\n",
    "    We wanna be strategic about how we pair up our training data.  Obviously \n",
    "    having just one whale of a class will not get us any generally useful features\n",
    "    for IDing the class.\n",
    "    \n",
    "    So this class will generate pairs of images in the form of a rank 2 tensor \n",
    "    with rows as pairs of jpeg file names.\n",
    "    '''\n",
    "    def __init__(self, train_filename, class_size_cutoff, rand_seed):\n",
    "        '''\n",
    "        train: filename of CSV (str)\n",
    "        class_size_cutoff: classes with this many images or less will get paired with every\n",
    "                            image.\n",
    "        rand_seed: included so we can randomly generate the same pairs for training\n",
    "        '''\n",
    "        r.seed(rand_seed)\n",
    "        \n",
    "        self.train = pd.read_csv(train_filename)\n",
    "        \n",
    "        whale_num = self.train.groupby('Id').size()\n",
    "        small_class_names = whale_num[whale_num <= class_size_cutoff].index.values\n",
    "        all_class_names = whale_num.index.values\n",
    "        self.small_class = self.train.set_index('Id').loc[small_class_names]\n",
    "        self.all_class = self.train.set_index('Id').loc[all_class_names]\n",
    "        \n",
    "        #np.random.shuffle(self.small_classes['Image'])\n",
    "        #np.random.shuffle(self.big_classes['Image'])\n",
    "        self.small_size = self.small_class.shape[0]\n",
    "        self.all_size = self.all_class.shape[0]\n",
    "        self.remaining_small_class = self.small_class['Image'][:]\n",
    "        self.remaining_all_class = self.all_class['Image'][:] \n",
    "        self.current_img = np.random.choice(self.remaining_small_class, replace = False)\n",
    "        self.current_class = self.train[self.train['Image'] \n",
    "                                        == self.current_img]['Id'].values[0]\n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "    \n",
    "    def get_pair(self):\n",
    "        '''\n",
    "        returns a dict corresponding to the jpegs\n",
    "        \n",
    "        we write the generator like this so that instead of generating a gigantic numpy \n",
    "        array and storing it in RAM \n",
    "        '''\n",
    "        while self.i <= self.small_size:\n",
    "            other_class_img = np.random.choice(self.remaining_all_class, replace = False)\n",
    "            other_class = self.train[self.train['Image'] == \n",
    "                                     other_class_img]['Id'].values[0]\n",
    "            similarity = np.float(other_class == self.current_class)\n",
    "            d = {'small class' : (self.current_img, self.current_class),\n",
    "                  'other class' : (other_class_img, other_class),\n",
    "                'similarity' : similarity}\n",
    "            if self.j < self.all_size - 1:\n",
    "                self.j += 1\n",
    "            else:\n",
    "                self.i += 1\n",
    "                self.j = 0 \n",
    "                self.remaining_all_class = self.all_class['Image'][:]\n",
    "                self.current_img = np.random.choice(\n",
    "                                    self.remaining_small_class, replace = False)\n",
    "                self.current_class = self.train[self.train['Image'] \n",
    "                                                == self.current_img]['Id'].values[0]\n",
    "            yield d       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'other class': ('5e7beef6e.jpg', 'w_5f20f03'),\n",
       " 'similarity': 0.0,\n",
       " 'small class': ('564df4f6c.jpg', 'w_71ec6d4')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = TrainingMatcher('train.csv', 2, 12)\n",
    "x = y.get_pair()\n",
    "x.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataPipeline:\n",
    "    '''\n",
    "    imports the images using tensorflow fed by instances of the generator in\n",
    "    the TrainingMatcher class\n",
    "    \n",
    "    alsos it will zero pad the outclasss\n",
    "    '''\n",
    "    def __init__(self, filename, cutoff, seed, height, width):\n",
    "        self.matcher = TrainingMatcher(filename, cutoff, seed)\n",
    "        self.next_pair = self.matcher.get_pair\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "    \n",
    "    def image_map(self, image_dict):\n",
    "        '''\n",
    "        use the previous class to read in two images and turn them into \n",
    "        tensorflow images\n",
    "        '''\n",
    "        file_1 = tf.io.read_file('/Users/jbowen/projects/train/' \n",
    "                                 + image_dict['small class'][0])\n",
    "        file_2 = tf.io.read_file('/Users/jbowen/projects/train/' \n",
    "                                 + image_dict['other class'][0])\n",
    "        img_1 = tf.image.decode_jpeg(file_1,channels=3)\n",
    "        img_2 = tf.image.decode_jpeg(file_2,channels=3)\n",
    "\n",
    "        img_1 = tf.image.resize_image_with_pad(img_1, self.height, self.width)\n",
    "        img_2 = tf.image.resize_image_with_pad(img_2, self.height, self.width)\n",
    "        return tf.stack([img_1, img_2], axis = 0)\n",
    "    \n",
    "    def get_similarity(self, dataset_dict):\n",
    "        '''\n",
    "        functiom to map over dataset to build dataset with class informatio \n",
    "        '''\n",
    "        return dataset_dict['similarity']\n",
    "        \n",
    "    def build_dataset(self, batch_size):\n",
    "        '''\n",
    "        batch_size : int, size to data to read before updating weights\n",
    "        '''\n",
    "        dataset = tf.data.Dataset.from_generator(self.next_pair, output_types = \n",
    "                                        {'other class': tf.string, 'small class': tf.string,\n",
    "                                        'similarity': tf.float16})\n",
    "        X_data = dataset.map(self.image_map, num_parallel_calls = 4)\n",
    "        X_data = X_data.batch(batch_size)\n",
    "        Y_data = dataset.map(self.get_similarity, num_parallel_calls = 4)\n",
    "        Y_data = Y_data.batch(batch_size)\n",
    "        return (X_data, Y_data)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b0db114f332f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "#executed with eager mode enabled\n",
    "import matplotlib.pyplot as plt\n",
    "x = DataPipeline('train.csv', 4, 1234, 1200, 800)\n",
    "y = x.build_dataset(20)[0].make_one_shot_iterator()\n",
    "i = y.get_next()\n",
    "f = plt.figure()\n",
    "f.add_subplot(1,2,1)\n",
    "plt.imshow(i.numpy()[0])\n",
    "f.add_subplot(1,2,2)\n",
    "plt.imshow(i.numpy()[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k\n",
    "from keras.models import Sequential, Model\n",
    "from keras.activations import sigmoid, relu\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Subtract, Flatten\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.engine.input_layer import Input\n",
    "\n",
    "\n",
    "def loss_function(prediction, distance):\n",
    "    #loss function from Koch's masters thesis\n",
    "    #prediction is 0 or 1 \n",
    "    loss = prediction * np.log(distance) + (1 - prediction) * np.log(1 - distance) \n",
    "    return loss\n",
    "                 \n",
    "def distance_function(component_weight, output_1, output_2):\n",
    "    return sigmoid(np.abs(output_1 - output_2).component_weight)\n",
    "\n",
    "#following functions decprecated 1/22/19, subbing in builtin Keras initializers\n",
    "\n",
    "#def init_weights(shape):\n",
    "#    values = np.random.normal(loc = 0, scale = 1e-2, size = shape)\n",
    "#    return k.variable(values)\n",
    "\n",
    "#def init_bias(shape):\n",
    "#    values = np.normal(loc = 0.5, scale = 1e-2, size = shape)\n",
    "#    return k.variable(values)\n",
    "\n",
    "class CnnSiameseNetwork:\n",
    "    '''\n",
    "    let's start by hard-coding in the CNN architecture from Koch\n",
    "    \n",
    "    all the hard coded numbers come from http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf 14-15\n",
    "    '''\n",
    "    def __init__(self, X_dataset, Y_dataset):\n",
    "        '''\n",
    "        Pipeline is an instance of the DataPipeline class from the earlier page\n",
    "        '''\n",
    "        \n",
    "        X_dataset_0 =  X_dataset.map(lambda x: x[0], num_parallel_calls=4)\n",
    "        X_dataset_1 =  X_dataset.map(lambda x: x[1], num_parallel_calls=4)\n",
    "        \n",
    "        self.Input_0 = Input(tensor = X_dataset_0.make_one_shot_iterator().get_next())\n",
    "        self.Input_1 = Input(tensor = X_dataset_1.make_one_shot_iterator().get_next())\n",
    "        \n",
    "        self.twin_cnn = Sequential()\n",
    "        input_sh = tuple(X_dataset.output_shapes.as_list()[2:4]) + (3,)\n",
    "        print input_sh\n",
    "        \n",
    "        self.twin_cnn.add(Conv2D(64, (10, 10), activation = 'relu', \n",
    "                                 bias_initializer = RandomNormal(mean = 0.5, stddev = 1e-2),\n",
    "                                 input_shape=input_sh,  data_format = 'channels_last',\n",
    "                              kernel_initializer = RandomNormal(mean = 0.0, stddev = 1e-2),\n",
    "                                 kernel_regularizer = l2(2e-4))\n",
    "                         )\n",
    "        self.twin_cnn.add(MaxPooling2D())\n",
    "        self.twin_cnn.add(Conv2D(128, (7, 7), \n",
    "                                 bias_initializer = RandomNormal(mean = 0.5, stddev = 1e-2), \n",
    "                                 kernel_initializer = RandomNormal(mean = 0.0, stddev = 1e-2), \n",
    "                              kernel_regularizer = l2(2e-4), activation = 'relu')\n",
    "                         )\n",
    "        self.twin_cnn.add(MaxPooling2D())\n",
    "        self.twin_cnn.add(Conv2D(128, (4, 4), activation = 'relu', \n",
    "                                 bias_initializer = RandomNormal(mean = 0.5, stddev = 1e-2),\n",
    "                              kernel_initializer = RandomNormal(mean = 0.0, stddev = 1e-2),\n",
    "                                 kernel_regularizer = l2(2e-4))\n",
    "                         )\n",
    "        self.twin_cnn.add(MaxPooling2D())\n",
    "        self.twin_cnn.add(Conv2D(256, (4, 4), activation = 'relu', \n",
    "                                 bias_initializer = RandomNormal(mean = 0.5, stddev = 1e-2),\n",
    "                              kernel_initializer = RandomNormal(mean = 0.0, stddev = 1e-2), \n",
    "                                 kernel_regularizer = l2(2e-4))\n",
    "                         )\n",
    "        self.twin_cnn.add(Flatten())\n",
    "        self.twin_cnn.add(Dense(4096, activation = 'sigmoid', \n",
    "                                bias_initializer = RandomNormal(mean = 0.5, stddev = 1e-2),\n",
    "                              kernel_initializer = RandomNormal(mean = 0.0, stddev = 1e-2), \n",
    "                                kernel_regularizer = l2(2e-4))\n",
    "                         )\n",
    "        \n",
    "        self.twin0_output = self.twin_cnn(self.Input_0)\n",
    "        self.twin1_output = self.twin_cnn(self.Input_1)\n",
    "        \n",
    "        self.diff = Subtract()([self.twin0_output,self.twin1_output])\n",
    "        self.embedded_layer = Dense(1, activation = 'sigmoid', \n",
    "                                    bias_initializer = RandomNormal(mean = 0.5, stddev = 1e-2))(self.diff)\n",
    "        \n",
    "        self.model = Model(input = [self.Input_0, self.Input_1], output = self.embedded_layer)\n",
    "        \n",
    "    \n",
    "    def compile_model(self, learning_rate, momentum, decay):\n",
    "        '''\n",
    "        learning rate: float64 to control rate at which weights are updated in the standard stochastic\n",
    "            gradient descent algo\n",
    "        momentum: normal float64 damping factor on parameter changes\n",
    "        decay: normal float64 damping factor on changes to learning rate across epochs\n",
    "        '''\n",
    "        sgd = k.optimizers.SGD(lr = learning_rate, momentum = momentum, decay = decay)\n",
    "        self.model.compile(loss = \"binary_crossentropy\", optimizer = sgd) \n",
    "        \n",
    "    def train_model(self, batch, num_epochs):\n",
    "        '''\n",
    "        batch: size of batch to go through dataset with\n",
    "        epochs: number of times to go over training data\n",
    "        '''\n",
    "        self.model.fit(batch_size = batch, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DataPipeline('train.csv', 12, 1234, 1200, 800)\n",
    "l = x.build_dataset(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 800, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbowen/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:88: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "A = CnnSiameseNetwork(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.compile_model(.05, .9, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
