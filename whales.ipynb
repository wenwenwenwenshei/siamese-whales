{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so there are a lot of whales that we have just one of\n",
    "#let's see how many of these bad boys are compared to the total\n",
    "whales = train.groupby('Id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.670929070929071"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "to get a better idea of what we;re dealing with let's make a histrogram of the whales\n",
    "'''\n",
    "\n",
    "\n",
    "float(whales[whales <= 2].size)/float(whales.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nso it's pretty damn impressive if we can even \\nget 60% of these, seeing as we only have 1 for many of the individuals\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so it's pretty damn impressive if we can even \n",
    "get 60% of these, seeing as we only have 1 for many of the individuals\n",
    "\n",
    "If we're going to learn meaningful features about individuals with only \n",
    "one picture, we can only really learn differences in a meaningful way.  For\n",
    "example, if we see the right side of a fluke, we won't be able to say \n",
    "much about the left side of a fluke of the same individual if say, the whale\n",
    "was partially eaten by a shark.  Although learning differences won;t get us\n",
    "around an example as pedantic as this, it will outperform a standard feature\n",
    "learning network \n",
    "\n",
    "http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf Koch's masters thesis \n",
    "describes training on MNIST data and applying affine transformations.  In \n",
    "theory it could be useful to apply a number of transformations to the whales\n",
    "to see what could happen, but in the first round of training, for the sake, we will \n",
    "design a pipeline that just grayscales the data and randomly selects a pair of a whale \n",
    "with only one member and pairs it with a whale we have multiples of. \n",
    "\n",
    "I'm going to make a class that will let us determine which images to pair and store \n",
    "the pairs as strings we can load into \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.image as img\n",
    "import keras as k\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "class Training_Matcher:\n",
    "    '''\n",
    "    We wanna be strategic about how we pair up our training data.  Obviously \n",
    "    having just one whale of a class will not do for pure feature recognition.\n",
    "    \n",
    "    So this class will generate pairs of images in the form of a rank 2 tensor \n",
    "    with rows as pairs of jpeg file names.\n",
    "    '''\n",
    "    def __init__(self, train_filename, class_size_cutoff, rand_seed):\n",
    "        '''\n",
    "        train: filename of CSV (str)\n",
    "        class_size_cutoff: classes with this many images or less will get paired with every\n",
    "                            image.\n",
    "        rand_seed: included so we can randomly generate the same pairs for training\n",
    "        '''\n",
    "        r.seed(rand_seed)\n",
    "        \n",
    "        self.train = pd.read_csv(train_filename)\n",
    "        \n",
    "        \n",
    "        whale_num = self.train.groupby('Id').size()\n",
    "        small_classes_names = whale_num[whale_num <= class_size_cutoff].index.values\n",
    "        big_classes_names = whale_num[whale_num > class_size_cutoff].index.values\n",
    "        self.small_classes = self.train.set_index('Id').loc[small_classes_names]\n",
    "        self.big_classes = self.train.set_index('Id').loc[big_classes_names]\n",
    "        \n",
    "        #np.random.shuffle(self.small_classes['Image'])\n",
    "        #np.random.shuffle(self.big_classes['Image'])\n",
    "        self.small_size = self.small_classes.shape[0]\n",
    "        self.big_size = self.big_classes.shape[0]\n",
    "        self.remaining_small_classes = self.small_classes['Image'][:]\n",
    "        self.remaining_big_classes = self.big_classes['Image'][:] \n",
    "        self.current_img = np.random.choice(self.remaining_small_classes, replace = False)\n",
    "        \n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "    \n",
    "    def get_pair(self):\n",
    "        '''\n",
    "        returns a dict corresponding to the jpegs\n",
    "        \n",
    "        we write the generator like this so that instead of generating a gigantic numpy \n",
    "        array and storing it in RAM \n",
    "        '''\n",
    "        while self.i <= self.small_size:\n",
    "            d = {'small class':self.current_img, \n",
    "                  'big class':np.random.choice(self.remaining_big_classes, replace = False)}\n",
    "            if self.j < self.big_size - 1:\n",
    "                self.j += 1\n",
    "            else:\n",
    "                self.i += 1\n",
    "                self.j = 0 \n",
    "                big_classes_copy = self.big_classes['Image'][:]\n",
    "                self.current_img = np.random.choice(\n",
    "                                    self.remaining_small_classes, replace = False)\n",
    "            yield d       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Training_Matcher('train.csv', 2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'large class': '9a2968eae.jpg', 'small class': '8a6c28487.jpg'}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get_pair().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataPipeline:\n",
    "    '''\n",
    "    imports the images using tensorflow fed by instances of the generator in\n",
    "    the TrainingMatcher class\n",
    "    \n",
    "    alsos it will zero pad the outclasss\n",
    "    '''\n",
    "    def __init__(self, filename, cutoff, seed, height, width):\n",
    "        self.next_pair = Training_Matcher(filename, cutoff, seed)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "    \n",
    "    def image_map(self, image_dict):\n",
    "        '''\n",
    "        use the previous class to read in two images and turn them into \n",
    "        tensorflow images\n",
    "        \n",
    "        we're going to do some downsampling here to speed things up as tf.decode_jpeg does\n",
    "        really fast downsampling\n",
    "        '''\n",
    "        file_1 = tf.io.read_file(image_dict['small_class'])\n",
    "        file_2 = tf.io.read_file(image_dict['big_class'])\n",
    "        img_1 = tf.image.decode_jpeg(file_1, ratio = 2)\n",
    "        img_2 = tf.image.decode_jpeg(file_2, ratio = 2)\n",
    "        \n",
    "        img_1 = tf.image.resize_image_with_pad(img_1, self.height, self.width)\n",
    "        img_2 = tf.image.resize_image_with_pad(img_2, self.height, self.width)\n",
    "        \n",
    "        return np.asarray([img_1, img_2])\n",
    "        \n",
    "    def build_dataset(self, batch_size):\n",
    "        dataset = tf.data.Dataset.from_generator(self.next_pair)\n",
    "        dataset = dataset.map(image_map, num_parallel_calls = 4)\n",
    "        dataset = dataset.shuffle()\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k\n",
    "from keras.models import Sequential, Model\n",
    "from keras.activations import sigmoid, relu\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Subtract\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def loss_function(prediction, distance):\n",
    "    #loss function from Koch's masters thesis\n",
    "    #prediction is 0 or 1 \n",
    "    loss = prediction * np.log(distance) + (1 - prediction) * np.log(1 - distance) \n",
    "    return loss\n",
    "                 \n",
    "def distance_function(component_weight, output_1, output_2):\n",
    "    return sigmoid(np.abs(output_1 - output_2).component_weight)\n",
    "\n",
    "def init_weights(shape):\n",
    "    values = np.normal(loc = 0, scale = 1e-2, size = shape)\n",
    "    return k.variable(values)\n",
    "\n",
    "def init_bias(shape):\n",
    "    values = np.normal(loc = 0.5, scale = 1e-2, size = shape)\n",
    "    return k.variable(values)\n",
    "\n",
    "class cnn_siamese_network:\n",
    "    '''\n",
    "    let's start by hard-coding in the CNN architecture from Koch\n",
    "    \n",
    "    all the hard coded numbers come from http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf 14-15\n",
    "    '''\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(64, (10, 10), activation = 'relu', bias_inializer = init_bias\n",
    "                              kernel_initializer = init_weights, kernel_regularizer = l2(2e-4)))\n",
    "        self.model.add(MaxPooling2D())\n",
    "        self.model.add(Conv2D(128, (7, 7), bias_inializer = init_bias, kernel_initializer = init_weights, \n",
    "                              kernel_regularizer = l2(2e-4), activation = 'relu'))\n",
    "        self.model.add(MaxPooling2D())\n",
    "        self.model.add(Conv2D(128, (4, 4), activation = 'relu', bias_inializer = init_bias\n",
    "                              kernel_initializer = init_weights, kernel_regularizer = l2(2e-4)))\n",
    "        self.model.add(MaxPooling2D())\n",
    "        self.model.add(Conv2D(256, (4, 4), activation = 'relu'), bias_inializer = init_bias\n",
    "                              kernel_initializer = init_weights, kernel_regularizer = l2(2e-4))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(4096, activation = 'sigmoid', bias_inializer = init_bias\n",
    "                              kernel_initializer = init_weights, kernel_regularizer = l2(2e-4)))\n",
    "    \n",
    "    def predict_input(self, input_tensor_1, input_tensor_2):\n",
    "        '''\n",
    "        get us the similarity score as per the L_1 distance between the two predictions\n",
    "        not sure how I want to batch the training as I write this, so I'm going to intentionally \n",
    "        leave the size of input_tensor_1, the input tensor, ambiguous. \n",
    "        \n",
    "        input_tensor_1/2: tensor referring to one batch of inputs, of size input_shape\n",
    "        '''\n",
    "        output_tensor_1 = self.model(input_tensor_1)\n",
    "        output_tensor_2 = self.model(input_tensor_2)\n",
    "        self.siamese_network = Model(inputs = [output_tensor_1,output_tensor_2], \n",
    "                                    )\n",
    "        k.backend.abs(Subtract(output_tensor_1, output_tensor_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- B/w images\\n- edit generator to include same/diff class label\\n- edit generator to pair same class items from large classes\\n'"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- B/w images\n",
    "- edit generator to include same/diff class label\n",
    "- edit generator to pair same class items from large classes\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
